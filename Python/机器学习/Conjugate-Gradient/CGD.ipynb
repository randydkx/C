{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from CG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.matrix([[3.0, 2.0], [2.0, 6.0]])\n",
    "b = np.matrix([[2.0], [-8.0]])\n",
    "c = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D Homework <a id=\"D\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following questions, assume (unless otherwise stated) that you are using exact arithmetic; there is no floating point roundoff error.\n",
    "1. (Easy.) Prove that the preconditioned Conjugate Gradient method is not affected if the preconditioning matrix is scaled. In other words, for any nonzero constant $\\gamma$, show that the sequence of steps $x_{[0]}, x_{[1]}, \\dots$ taken using the preconditioner $\\gamma M$ is identical to the steps taken using the preconditioner $M$.\n",
    "\n",
    "2. (Hard, but interesting.) Suppose you wish to solve $Ax = b$ for a symmetric, positive-definite $n \\times n$ matrix $A$. Unfortunately, the trauma of your linear algebra course has caused you to repress all memories of the Conjugate Gradient algorithm. Seeing you in distress, the Good Eigenfairy materializes and grants you a list of the $d$ distinct eigenvalues (but not the eigenvectors) of $A$. However, you do _not_ know how many times each eigenvalue is repeated.\n",
    "\n",
    "Clever person that you are, you mumbled the following algorithm in your sleep this morning, using an arbitrary starting point ```x```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2 # or whatever dimensions you are given\n",
    "x = np.matrix([[-2.,],[-2.]])\n",
    "ev = np.array([1,2]) # or whatever the list you are given is\n",
    "for i in range(d):\n",
    "    r = b - A * x\n",
    "    x = x + r/ev[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No eigenvalue is used twice; on termination, the list is empty.\n",
    "\n",
    "(a) Show that upon termination of this algorithm, $x_{[d]}$ is the solution to $Ax = b$. __Hint:__ Read Section\n",
    "6.1 carefully. Graph the convergence of a two-dimensional example; draw the eigenvector axes, and try selecting each eigenvalue first. (It is most important to intuitively explain what the algorithm is doing; if you can, also give equations that prove it.)\n",
    "\n",
    "(b) Although this routine finds the exact solution after $d$ iterations, you would like each intermediate iterate $x_{[i]}$ to be as close to the solution as possible. Give a _crude_ rule of thumb for how you should choose an eigenvalue from the list on each iteration. (In other words, in what order should the eigenvalues be used?)\n",
    "\n",
    "(c) What could go terribly wrong with this algorithm if floating point roundoff occurs?\n",
    "\n",
    "(d) Give a rule of thumb for how you should choose an eigenvalue from the list on each iteration to prevent floating point roundoff error from escalating. __Hint__: The answer is not the same as that of question (b)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
