{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from CG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.matrix([[3.0, 2.0], [2.0, 6.0]])\n",
    "b = np.matrix([[2.0], [-8.0]])\n",
    "c = 0.0\n",
    "x = np.matrix([[-2.0], [-2.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Canned Algorithms <a id=\"B\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code given in this section represents efficient implementations of the algorithms discussed in this article.\n",
    "\n",
    "<html>\n",
    "<hr style=\"height:2px;border:none;color:#228;background-color:#228;\" />\n",
    "<span style=\"color:#228\">Note, these have been converted from the original pseudocode to Python functions by AvS.<hr style=\"height:2px;border:none;color:#228;background-color:#228;\" />\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.1 Steepest Descent <a id=\"B.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the inputs ```A, b```, a starting value ```x```, a maximum number of iterations ```imax```, and an error tolerance ```epsilon``` <1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SD(A, b, x, imax=10, epsilon = 0.01):\n",
    "    steps=np.asarray(x)\n",
    "    i = 0\n",
    "    r = b - A * x\n",
    "    delta = r.T * r\n",
    "    delta0 = delta\n",
    "    while i < imax and delta > epsilon**2 * delta0:\n",
    "        q = A * r\n",
    "        alpha = float(delta / (r.T * q))\n",
    "        x = x + alpha * r\n",
    "        if i%50 == 0:\n",
    "            r = b - A * x\n",
    "        else:\n",
    "            r = r - alpha * q\n",
    "        delta = r.T * r\n",
    "        i = i + 1\n",
    "        steps = np.append(steps, np.asarray(x), axis=1)\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f03e0e0e1ee41eb8e0a11e2ced3a5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = SD(A,b,x)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8), num='Figure B.1')\n",
    "plotcontours(A, b, c, fig)\n",
    "plt.plot(steps[0,:], steps[1,:], '-mo');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm terminates when the maximum number of iterations ```imax``` has been exceeded, or when $\\|r_{[i]}\\| \\leq $ ``` epsilon``` $\\|r_{[0]}\\|$.\n",
    "\n",
    "The fast recursive formula for the residual is usually used, but once every 50 iterations, the exact residual is recalculated to remove accumulated floating point error. Of course, the number 50 is arbitrary; for large $n, \\sqrt{n}$ might be appropriate. If the tolerance is large, the residual need not be corrected at all (in practice, this correction is rarely used). If the tolerance is close to the limits of the floating point precision of the machine, a test should be added after ```delta``` is evaluated to check if ```delta <= epsilon**2 * delta0```, and if this test holds true, the exact residual should also be recomputed and ```delta``` reevaluated. This prevents the procedure from terminating early due to floating point roundoff error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 Conjugate Gradients <a id=\"B.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the inputs ```A, b```, a starting value ```x```, a maximum number of iterations ```imax```, and an error tolerance ```epsilon``` < 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CG(A, b, x, imax=10, epsilon = 0.01):\n",
    "    steps=np.asarray(x)\n",
    "    i = 0\n",
    "    r = b - A * x\n",
    "    d = r.copy()\n",
    "    delta_new = r.T * r\n",
    "    delta_0 = delta_new\n",
    "    while i < imax and delta_new > epsilon**2 * delta_0:\n",
    "        q = A * d\n",
    "        alpha = float(delta_new / (d.T * q))\n",
    "        x = x + alpha * d\n",
    "        if i%50 == 0:\n",
    "            r = b - A * x\n",
    "        else:\n",
    "            r = r - alpha * q\n",
    "        delta_old = delta_new\n",
    "        delta_new = r.T * r\n",
    "        beta = float(delta_new / delta_old)\n",
    "        d = r + beta * d\n",
    "        i = i + 1\n",
    "        steps = np.append(steps, np.asarray(x), axis=1)\n",
    "    return steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8685fbd8355b4155ba355c7e99382f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = CG(A,b,x)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8), num='Figure B.2')\n",
    "plotcontours(A, b, c, fig)\n",
    "plt.plot(steps[0,:], steps[1,:], '-mo');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the comments at the end of Section B1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3 Preconditioned Conjugate Gradients <a id=\"B.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the inputs ```A, b```, a starting value ```x```, the inverse of a (perhaps implicitly defined) preconditioner ```M```, a maximum number of iterations ```imax```, and an error tolerance ```epsilon``` < 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCG(A, b, x, M_inv, imax=10, epsilon = 0.01):\n",
    "    steps=np.asarray(x)\n",
    "    i = 0\n",
    "    r = b - A * x\n",
    "    d = M_inv * r\n",
    "    delta_new = r.T * d\n",
    "    delta_0 = delta_new\n",
    "    while i < imax and delta_new > epsilon**2 * delta_0:\n",
    "        q = A * d\n",
    "        alpha = float(delta_new / (d.T * q))\n",
    "        x = x + alpha * d\n",
    "        if i%50 == 0:\n",
    "            r = b - A * x\n",
    "        else:\n",
    "            r = r - alpha * q\n",
    "        s = M_inv * r\n",
    "        delta_old = delta_new\n",
    "        delta_new = r.T * s\n",
    "        beta = float(delta_new / delta_old)\n",
    "        d = s + beta * d\n",
    "        i = i + 1\n",
    "        steps = np.append(steps, np.asarray(x), axis=1)\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40f1e3bd8474f22ad4c640aa0a772e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "M_inv = np.matrix([[1/3.0, 0], [0, 1/6.0]])\n",
    "steps = PCG(A,b,x,M_inv)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8), num='Figure B.3')\n",
    "plotcontours(A, b, c, fig)\n",
    "plt.plot(steps[0,:], steps[1,:], '-mo');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statement “```s = M_inv * r```” implies that one should apply the preconditioner, which may not actually be in the form of a matrix.\n",
    "\n",
    "See also the comments at the end of Section B1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.4 Nonlinear Conjugate Gradients with Newton-Raphson and Fletcher-Reeves <a id=\"B.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a function $f$, it's derivative ```df```, and second derivative ```ddf```, a starting value ```x```, a maximum number of CG iterations ```imax```, a CG error tolerance ```epsilon``` < 1, a maximum number of Newton-Raphson iterations ```jmax```, a Newton-Raphson error tolerance ```eps``` < 1, and a restart number ```n```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE HAS NOT BEEN TESTED\n",
    "def NCG_NRFR(df, ddf, x, imax=10, epsilon = 0.01, jmax=10, eps = 0.01, n=50):\n",
    "    # df and ddf must be callable functions\n",
    "    steps=np.asarray(x)\n",
    "    i = 0\n",
    "    k = 0\n",
    "    r = -df(x)\n",
    "    d = r.copy()\n",
    "    delta_new = r.T * r\n",
    "    delta_0 = delta_new\n",
    "    while i < imax and delta_new > epsilon**2 * delta0:\n",
    "        j = 0\n",
    "        delta_d = d.T * d\n",
    "        alpha = - float(df(x).T * d / (d.T * ddf(x) * d))\n",
    "        while j < jmax and alpha**2 * delta_d > eps**2:            \n",
    "            x = x + alpha * d\n",
    "            j = j + 1\n",
    "            alpha = - float(df(x).T * d / (d.T * ddf(x) * d))\n",
    "        r = - df(x)\n",
    "        delta_old = delta_new\n",
    "        delta_new = r.T * r\n",
    "        beta = float(delta_new / delta_old)\n",
    "        d = r + beta * d\n",
    "        k = k + 1\n",
    "        if k==n or r.T * d<=0:\n",
    "            d = r.copy()\n",
    "            k = 0\n",
    "        i = i + 1\n",
    "        steps = np.append(steps, np.asarray(x), axis=1)\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm terminates when the maximum number of iterations ```imax``` has been exceeded, or when $\\|r_{[i]}\\| \\leq$ ``` epsilon``` $r\\|_{[0]}\\|$.\n",
    "\n",
    "Each Newton-Raphson iteration adds ```alpha*d``` to ```x```; the iterations are terminated when each update ```alpha*d``` falls below a given tolerance (||```alpha*d```|| $\\leq$ ```eps```), or when the number of iterations exceeds ```jmax```. A fast inexact line search can be accomplished by using a small ```jmax``` and/or by approximating the Hessian ```ddf(x)``` with its diagonal.\n",
    "\n",
    "Nonlinear CG is restarted (by setting ```d = r```) whenever a search direction is computed that is not a descent direction. It is also restarted once every $n$ iterations, to improve convergence for small $n$.\n",
    "\n",
    "The calculation of ```alpha``` may result in a divide-by-zero error. This may occur because the starting point ```x``` is not sufficiently close to the desired minimum, or because is not twice continuously differentiable. In the former case, the solution is to choose a better starting point or a more sophisticated line search. In the latter case, CG might not be the most appropriate minimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.5 Preconditioned Nonlinear Conjugate Gradients with Secant and Polak-Ribière <a id=\"B.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a function $f$, it's derivative ```df```, the inverse ```M_inv``` of preconditioner $M \\approx f''(x)$, a starting value ```x```, a maximum number of CG iterations ```imax```, a CG error tolerance ```epsilon``` < 1, a Secant method step parameter ```sigma0```, a maximum number of Secant method iterations ```jmax```, and a Secant method error tolerance ```eps``` < 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE HAS NOT BEEN TESTED\n",
    "def PNCG_SPR(df, M_inv, x, imax=10, epsilon = 0.01, sigma0=0.1, jmax=10, eps = 0.01):\n",
    "    # df and M_inv must be callable functions\n",
    "    steps=np.asarray(x)\n",
    "    i = 0\n",
    "    k = 0\n",
    "    r = -df(x)\n",
    "    s = M_inv(x) * r\n",
    "    d = s.copy()\n",
    "    delta_new = r.T * d\n",
    "    delta_0 = delta_new\n",
    "    while i < imax and delta_new > epsilon**2 * delta0:\n",
    "        j = 0\n",
    "        delta_d = d.T * d\n",
    "        alpha = -sigma0\n",
    "        eta_prev = df(x + sigma0*d).T * d\n",
    "        while j < jmax and alpha**2 * delta_d > eps**2:\n",
    "            eta = df(x).T * d\n",
    "            alpha = alpha * (eta / (eta_prev - eta))\n",
    "            x = x + alpha * d\n",
    "            eta_prev = eta\n",
    "            j = j + 1\n",
    "        r = - df(x)\n",
    "        delta_old = delta_new\n",
    "        delta_mid = r.T * s\n",
    "        beta = float((delta_new - delta_mid) / delta_old)\n",
    "        k = k + 1\n",
    "        if k==n or beta<=0:\n",
    "            d = s.copy()\n",
    "            k = 0\n",
    "        else:\n",
    "            d = s + beta * d\n",
    "        i = i + 1\n",
    "        steps = np.append(steps, np.asarray(x), axis=1)\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm terminates when the maximum number of iterations ```imax``` has been exceeded, or when $\\|r_{[i]}\\| \\leq$ ``` epsilon```$\\|r_{[0]}\\|$.\n",
    "\n",
    "Each Secant method iteration adds ```alpha*d``` to ```x```; the iterations are terminated when each update ```a*d``` falls below a given tolerance (```||a*d|| <= eps```), or when the number of iterations exceeds ```jmax```. A fast inexact line search can be accomplished by using a small ```jmax```. The parameter ```sigma0``` determines the value of $\\sigma$ in Equation 59 for the first step of each Secant method minimization. Unfortunately, it may be necessary to adjust this parameter \n",
    "to achieve convergence.\n",
    "\n",
    "The Polak-Ribière ```beta``` parameter is $\\frac{\\delta_{new} - \\delta_{mid}}{\\delta_old} = \\frac{r^T_{[i+1]}s_{[i+1]} - r^T_{[i+1]}s_{[i]}}{r^T_{[i]}s_{[i]}} = \\frac{r^T_{[i+1]}M^{-1}(r_{[i+1]} - r_{[i]})}{r^T_{[i]}M^{-1}r_{[i]}}$. Care must be taken that the preconditioner is always positive-definite. The preconditioner is not necessarily in the form of a matrix.\n",
    "\n",
    "Nonlinear CG is restarted (by setting ```d = r.copy()```) whenever the Polak-Ribière parameter ```beta``` is negative. It is also restarted once every ```n``` iterations, to improve convergence for small ```n```.\n",
    "\n",
    "Nonlinear CG presents several choices: Preconditioned or not, Newton-Raphson method or Secant method or another method, Fletcher-Reeves or Polak-Ribière. It should be possible to construct any of the variations from the versions presented above. (Polak-Ribière is almost always to be preferred.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: [C. Ugly Proofs](CGC.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
