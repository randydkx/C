{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from CG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.matrix([[3.0, 2.0], [2.0, 6.0]])\n",
    "b = np.matrix([[2.0], [-8.0]])\n",
    "c = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Convergence Analysis of Steepest Descent <a id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Instant Results <a id=\"6.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the convergence of Steepest Descent, let’s first consider the case where $e_{[i]}$ is an eigenvector with eigenvalue $\\lambda_e$. Then, the residual $r_{[i]} = -Ae_{[i]} = -\\lambda_e e_{[i]}$ is also an eigenvector. Equations 11 and 12 12 give\n",
    "\n",
    "\\begin{align*}\n",
    "\\alpha_{[i]} &= \\frac{r_{[i]}^Tr_{[i]}}{r_{[i]}^TAr_{[i]}}, \\tag{11} \\\\\n",
    "x_{[i+1]} &= x_{[i]} + \\alpha_{[i]}r_{[i]}. \\tag{12} \\\\\n",
    "x + e_{[i+1]} &= x + e_{[i]} + \\frac{r^T_{[i]}r_{[i]}}{r^T_{[i]}Ar_{[i]}}r_{[i]}\\\\\n",
    "e_{[i+1]} &= e_{[i]} + \\frac{r^T_{[i]}r_{[i]}}{\\lambda_e r^T_{[i]}r_{[i]}}(-\\lambda_e e_{[i]})\\\\\n",
    "&= 0.\n",
    "\\end{align*}\n",
    "\n",
    "Figure 14 demonstrates why it takes only one step to converge to the exact solution. The point lies on one of the axes of the ellipsoid, and so the residual points directly to the center of the ellipsoid. Choosing $\\alpha_{[i]}=\\lambda^{-1}_e$ gives us instant convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e2d21786d14620b80f75b43b75dce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig14()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 14: Steepest Descent converges to the exact solution on the first iteration if the error term is an eigenvector.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more general analysis, we must express $e_{[i]}$ as a linear combination of eigenvectors, and we shall furthermore require these eigenvectors to be orthonormal. It is proven in Appendix C2 that if $A$ is symmetric, there exists a set of $n$ orthogonal eigenvectors of $A$. As we can scale eigenvectors arbitrarily, let us choose so that each eigenvector is of unit length. This choice gives us the useful property that\n",
    "\n",
    "\\begin{equation}\n",
    "v^T_j v_k = \\begin{cases} 1, & j=k,\\\\ 0, & j \\neq k.\\end{cases} \\tag{17}\n",
    "\\end{equation}\n",
    "\n",
    "Express the error term as a linear combination of eigenvectors\n",
    "\n",
    "\\begin{equation}\n",
    "e_{[i]} = \\sum^n_{j=1}\\xi_j v_j, \\tag{18}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\xi_j$ is the length of each component of $e_{[i]}$. From Equations 17 and 18 we have the following identities:\n",
    "\n",
    "\\begin{align*}\n",
    "r_{[i]} &= -Ae_{[i]} = -\\sum_j \\xi_j \\lambda_j v_j, \\tag{19} \\\\\n",
    "\\|e_{[i]}\\|^2 = e^T_{[i]}e_{[i]} &= \\sum_j\\xi^2_j, \\tag{20} \\\\\n",
    "e^T_{[i]}Ae_{[i]} &= (\\sum_j \\xi_j v^T_j)(\\sum_j \\xi_j \\lambda_j v_j) \\\\\n",
    "&= \\sum_j \\xi^2_j \\lambda_j, \\tag{21}\\\\\n",
    "\\|r_{[i]}\\|^2 = r^T_{[i]}r_{[i]} &= \\sum_j \\xi^2_j \\lambda^2_j, \\tag{22}\\\\\n",
    "r^T_{[i]}Ar_{[i]} &= \\sum_j \\xi^2_j \\lambda^3_j. \\tag{23}\n",
    "\\end{align*}\n",
    "\n",
    "Equation 19 shows that $r_{[i]}$ too can be expressed as a sum of eigenvector components, and the length of these components are $-\\xi_j \\lambda_j$. Equations 20 and 22 are just Pythagoras’ Law.\n",
    "\n",
    "Now we can proceed with the analysis. Equation 12 gives\n",
    "\n",
    "\\begin{align*}\n",
    "e_{[i+1]} &= e_{[i]} + \\frac{r^T_{[i]}r_{[i]}}{r^T_{[i]}Ar_{[i]}}r_{[i]} \\\\\n",
    "&= e_{[i]} + \\frac{\\sum_j \\xi^2_j \\lambda^2_j}{\\sum_j \\xi^2_j \\lambda^3_j}r_{[i]} \\tag{24}\n",
    "\\end{align*}\n",
    "\n",
    "We saw in the last example that, if $e_{[i]}$ has only one eigenvector component, then convergence is achieved in one step by choosing $\\alpha_{[i]} = \\lambda^{-1}_e$. Now let’s examine the case where $e_{[i]}$ is arbitrary, but all the eigenvectors have a common eigenvalue $\\lambda$. Equation 24 becomes\n",
    "\n",
    "\\begin{align*}\n",
    "e_{[i+1]} &= e_{[i]} + \\frac{\\lambda^2 \\sum_j \\xi^2_j}{\\lambda^3 \\sum_j \\xi^2_j}(\\lambda e_{[i]})\\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "\n",
    "Figure 15 demonstrates why, once again, there is instant convergence. Because all the eigenvalues are equal, the ellipsoid is spherical; hence, no matter what point we start at, the residual must point to the center of the sphere. As before, choose $\\alpha_{[i]} = \\lambda^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204f44a0c39348f6b0765afa3eaefb70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig15()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 15: Steepest Descent converges to the exact solution on the first iteration if the eigenvalues are all equal</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if there are several unequal, nonzero eigenvalues, then no choice of $\\alpha_{[i]}$ will eliminate all the eigenvector components, and our choice becomes a sort of compromise. In fact, the fraction in Equation 24 is best thought of as a weighted average of the values of $\\lambda^{-1}_j$. The weights $\\xi^2_j$ ensure that longer components of $e_{[i]}$ are given precedence. As a result, on any given iteration, some of the shorter components of $e_{[i]}$ might actually *increase* in length (though never for long). For this reason, the methods of Steepest Descent and Conjugate Gradients are called *roughers*. By contrast, the Jacobi Method is a *smoother*, because every eigenvector component is reduced on every iteration. Steepest Descent and Conjugate Gradients are not smoothers, although they are often erroneously identified as such in the mathematical literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. General Convergence <a id=\"6.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6027baf6a6b94d2a8f0099d1d8222cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 16: The energy norm of these two vectors is equal.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To bound the convergence of Steepest Descent in the general case, we shall define the *energy norm* $\\|e\\|_A = (e^TAe)^{1/2}$ (see Figure 16). This norm is easier to work with than the Euclidean norm, and is in some sense a more natural norm; examination of Equation 8 shows that minimizing $\\|e_{[i]}\\|_A$ is equivalent to minimizing $f(x_{[i]})$. With this norm, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\|e_{[i+1]}\\|^2_A &= e^T_{[i+1]}Ae_{[i+1]} \\\\\n",
    "&= (e^T_{[i]} + \\alpha_{[i]}r^T_{[i]}) A (e_{[i]} + \\alpha_{[i]}r_{[i]}) \\qquad & \\text{(by Equation 12)}\\\\\n",
    "&= e^T_{[i]} A e_{[i]} + 2 \\alpha_{[i]} r^T_{[i]} A e_{[i]} + \\alpha^2_{[i]} r^T_{[i]} A r_{[i]} \\qquad & \\text{(by symmetry of A)}\\\\\n",
    "&= \\|e_{[i]}\\|^2_A + 2 \\frac{r^T_{[i]} r_{[i]}}{r^T_{[i]} A r_{[i]}} (-r^T_{[i]} r_{[i]}) + \\left(\\frac{r^T_{[i]} r_{[i]}}{r^T_{[i]} A r_{[i]}}\\right)^2 r^T_{[i]} A r_{[i]}\\\\\n",
    "&= \\|e_{[i]}\\|^2_A - \\frac{(r^T_{[i]} r_{[i]})^2}{r^T_{[i]} A r_{[i]}} \\\\\n",
    "&= \\|e_{[i]}\\|^2_A \\left(1 - \\frac{(r^T_{[i]} r_{[i]})^2}{(r^T_{[i]} A r_{[i]})(e^T_{[i]} A e_{[i]})}\\right) \\\\\n",
    "&= \\|e_{[i]}\\|^2_A \\left(1 - \\frac{(\\sum_j \\xi^2_j \\lambda^2_j)^2}{(\\sum_j \\xi^2_j \\lambda^3_j)(\\sum_j \\xi^2_j \\lambda_j)}\\right) \\qquad & \\text{(by Identities 21, 22, 23)} \\\\\n",
    "&= \\|e_{[i]}\\|^2_A \\omega^2, \\qquad \\omega^2 = 1 - \\frac{(\\sum_j \\xi^2_j \\lambda^2_j)^2}{(\\sum_j \\xi^2_j \\lambda^3_j)(\\sum_j \\xi^2_j \\lambda_j)} \\tag{25}\n",
    "\\end{align*}\n",
    "\n",
    "The analysis depends on finding an upper bound for $\\omega$. To demonstrate how the weights and eigenvalues affect convergence, I shall derive a result for $n=2$. Assume that $\\lambda_1 \\geq \\lambda_2$. The *spectral condition number* of $A$ is defined to be $\\kappa = \\lambda_1/\\lambda_2 \\geq 1$. The *slope* of $e_{[i]}$ (relative to the coordinate system defined by the eigenvectors), which depends on the starting point, is denoted $\\mu = \\xi_2/\\xi_1$. We have\n",
    "\n",
    "\\begin{align*}\n",
    "\\omega^2 &= 1 - \\frac{(\\xi^2_1 \\lambda^2_1 + \\xi^2_2 \\lambda^2_2)^2}{(\\xi^2_1 \\lambda^3_1 + \\xi^2_2 \\lambda^3_2)(\\xi^2_1 \\lambda_1 + \\xi^2_2 \\lambda_2)} \\\\\n",
    "&= 1 - \\frac{(\\kappa^2 + \\mu^2)^2}{(\\kappa + \\mu^2)(\\kappa^3 + \\mu^2)} \\tag{26}\n",
    "\\end{align*}\n",
    "\n",
    "The value of $\\omega$, which determines the rate of convergence of Steepest Descent, is graphed as a function of $\\mu$ and $\\kappa$ in Figure 17. The graph confirms my two examples. If $e_{[0]}$ is an eigenvector, then the slope $\\mu$ is zero (or infinite); we see from the graph that $\\omega$ is zero, so convergence is instant. If the eigenvalues are equal, then the condition number $\\kappa$ is one; again, we see that $\\omega$ is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ee0ca4db234ccb9dc8dcef8f3e94ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig17()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 17: Convergence $\\omega$ of Steepest Descent as a function of $\\mu$ (the slope of $e_{[i]}$) and $\\kappa$ (the condition number of $A$). Convergence is fast when $\\mu$ or $\\kappa$ are small. For a fixed matrix, convergence is worst when $\\mu = \\pm \\kappa$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 18 illustrates examples from near each of the four corners of Figure 17. These quadratic forms are graphed in the coordinate system defined by their eigenvectors. Figures 18(a) and 18(b) are examples with a large condition number. Steepest Descent can converge quickly if a fortunate starting point is chosen (Figure 18(a)), but is usually at its worst when $\\kappa$ is large (Figure 18(b)). The latter figure gives us our best intuition for why a large condition number can be bad: $f(x)$ forms a trough, and Steepest Descent bounces back and forth between the sides of the trough while making little progress along its length. In Figures 18(c) and 18(d), the condition number is small, so the quadratic form is nearly spherical, and convergence is quick regardless of the starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66deca708340471fbb46f5e3ae224f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig18()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 18: These four examples represent points near the corresponding four corners of the graph in Figure 17. (a) Large $\\kappa$, small $\\mu$. (b) An example of poor convergence. $\\kappa$ and $\\mu$ are both large. (c) Small $\\kappa$ and $\\mu$. (d) Small $\\kappa$, large $\\mu$.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holding $\\kappa$ constant (because $A$ is fixed), a little basic calculus reveals that Equation 26 is maximized when $\\mu = \\pm \\kappa$. In Figure 17, one can see a faint ridge defined by this line. Figure 19 plots worst-case starting points for our sample matrix $A$. These starting points fall on the lines defined by $\\xi_2/\\xi_1 = \\pm \\kappa$. An upper bound for $\\omega$ (corresponding to the worst-case starting points) is found by setting $\\mu^2 = \\kappa^2$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\omega^2 &\\leq 1 - \\frac{4\\kappa^4}{\\kappa^5 + 2\\kappa^4 + \\kappa^3} \\\\\n",
    "&= \\frac{\\kappa^5 - 2\\kappa^4 + \\kappa^3}{\\kappa^5 + 2\\kappa^4 + \\kappa^3} \\\\\n",
    "&= \\frac{(\\kappa - 1)^2}{(\\kappa + 1)^2} \\\\\n",
    "\\omega &\\leq \\frac{\\kappa - 1}{\\kappa + 1}. \\tag{27} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0038e80eed414028bea530c1d9f24a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig19()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 19: Solid lines represent the starting points that give the worst convergence for Steepest Descent. Dashed lines represent steps toward convergence. If the first iteration starts from a worst-case point, so do all succeeding iterations. Each step taken intersects the paraboloid axes (gray arrows) at precisely a 45° angle. Here, $\\kappa = 3.5$.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inequality 27 is plotted in Figure 20. The more *ill-conditioned* the matrix (that is, the larger its condition number $\\kappa$), the slower the convergence of Steepest Descent. In Section 9.2, it is proven that Equation 27 is also valid for $n>2$, if the condition number of a symmetric, positive-definite matrix is defined to be \n",
    "\n",
    "$$\n",
    "\\kappa = \\lambda_{max}/\\lambda_{min}\n",
    "$$\n",
    "\n",
    "the ratio of the largest to smallest eigenvalue. The convergence results for Steepest Descent are\n",
    "\n",
    "\\begin{align*}\n",
    "\\|e_{i]}\\|_A &\\leq \\left(\\frac{\\kappa-1}{\\kappa+1}\\right)^i \\| e_{[0]}\\|_A, \\qquad \\text{and} \\tag{28} \\\\\n",
    "\\frac{f(x_{i]}) - f(x)}{f(x_{[0]}) - f(x)} &= \\frac{\\frac{1}{2} e^t_{i]} A e_{i]}}{\\frac{1}{2} e^t_{[0]} A e_{[0]}} \\qquad \\text{by Equation 8} \\\\ \n",
    "&\\leq \\left(\\frac{\\kappa - 1}{\\kappa + 1}\\right)^{2i}. \\tag{29}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94af18e933c450085ca31ee693802a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig20()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 20: Convergence of Steepest Descent (per iteration) worsens as the condition number of the matrix increases.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: [7. The Method of Conjugate Directions](CG07.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
