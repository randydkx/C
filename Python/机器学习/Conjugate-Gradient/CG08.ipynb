{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from CG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.matrix([[3.0, 2.0], [2.0, 6.0]])\n",
    "b = np.matrix([[2.0], [-8.0]])\n",
    "c = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Method of Conjugate Gradients <a id=\"8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may seem odd that an article about CG doesn’t describe CG until section 8, but all the machinery is now in place. In fact, CG is simply the method of Conjugate Directions where the search directions are constructed by conjugation of the residuals (that is, by setting $\\mu_i = r_{[i]}$).\n",
    "\n",
    "This choice makes sense for many reasons. First, the residuals worked for Steepest Descent, so why not for Conjugate Directions? Second, the residual has the nice property that it’s orthogonal to the previous search directions (Equation 39), so it’s guaranteed always to produce a new, linearly independent search direction unless the residual is zero, in which case the problem is already solved. As we shall see, there is an even better reason to choose the residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a19e8ceb9e425b803f79d8943e8159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig29()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 29: In the method of Conjugate Gradients, each new residual is orthogonal to all the previous residuals and search directions; and each new search direction is constructed (from the residual) to be $A$-orthogonal to all the previous residuals and search directions. The endpoints of $r_{[2]}$ and $d_{[2]}$ lie on a plane parellel to $\\mathcal{D}_2$ (the shaded subspace). In CG, $d_{[2]}$ is a linear combination of $r_{[2]}$ and $d_{[1]}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s consider the implications of this choice. Because the search vectors are built from the residuals, the subspace span{$r_{[0]}, r_{[1]}, \\dots, r_{[i-1]}$} is equal to $\\mathcal{D}_i$. As each residual is orthogonal to the previous search directions, it is also orthogonal to the previous residuals (see Figure 29); Equation 41 becomes\n",
    "\n",
    "\\begin{equation}\n",
    "r^T_{[i]}r_{[j]} = 0, \\quad i\\neq j \\tag{44}\n",
    "\\end{equation}\n",
    "\n",
    "Interestingly, Equation 43 shows that each new residual $r_{[i]}$ is just a linear combination of the previous residual and $Ad_{[i-1]}$. Recalling that $d_{[i-1]} \\in \\mathcal{D}_i$, this fact implies that each new subspace $\\mathcal{D}_{i+1}$ is formed from the union of the previous subspace $\\mathcal{D}_i$ and the subspace $A\\mathcal{D}_i$. Hence,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{D}_i &= \\text{span\\{}d_{[0]}, Ad_{[0]}, A^2d_{[0]}, \\dots, A^{i-1}d_{[0]}\\}\\\\\n",
    "&= \\text{span\\{}r_{[0]}, Ar_{[0]}, A^2r_{[0]}, \\dots, A^{i-1}r_{[0]}\\}.\\\\\n",
    "\\end{align*}\n",
    "\n",
    "This subspace is called a *Krylov subspace*, a subspace created by repeatedly applying a matrix to a vector. It has a pleasing property: because $A\\mathcal{D}_i$ is included in $\\mathcal{D}_{i+1}$, the fact that the next residual $r_{[i+1]}$ is orthogonal to $\\mathcal{D}_{i+1}$ (Equation 39) implies that $r_{[i+1]}$ is $A$-orthogonal to $\\mathcal{D}_i$. Gram-Schmidt conjugation becomes easy, because $r_{[i+1]}$ is already $A$-orthogonal to all of the previous search directions except $d_{[i]}$!\n",
    "\n",
    "Recall from Equation 37 that the Gram-Schmidt constants are $\\beta_{ij} = -r^T_{[i]}Ad_{[j]}/d^T_{[j]}Ad_{[j]}$; let us simplify this expression. Taking the inner product of $r_{[i]}$ and Equation 43,\n",
    "\n",
    "\\begin{align*}\n",
    "r^T_{[i]}r_{[j+1]} &= r^T_{[i]}r_{[j]} - \\alpha_{[j]} r^T_{[i]} A d_{[j]} \\\\\n",
    "\\alpha_{[j]} r^T_{[i]} A d_{[j]} &= r^T_{[i]}r_{[j]} - r^T_{[i]}r_{[j+1]} \\\\\n",
    "r^T_{[i]}Ad_{[j]} &=\n",
    "    \\begin{cases}\n",
    "    \\frac{1}{\\alpha_{[i]}}r^T_{[i]}r_{[i]}, & i=j,\\\\\n",
    "    -\\frac{1}{\\alpha_{[i-1]}}r^T_{[i]}r_{[i]}, & i=j+1, & \\text{(by Equation 44)}\\\\\n",
    "    0, & \\text{otherwise.}\n",
    "    \\end{cases} \\\\  \n",
    "\\therefore \\beta_{ij} &=\n",
    "    \\begin{cases}\n",
    "    \\frac{1}{\\alpha_{[i-1]}}\\frac{r^T_{[i]}r_{[i]}}{d^T_{[i-1]}Ad_{[i-1]}}, & i = j+1\\\\\n",
    "    0, & i > j+1.\n",
    "    \\end{cases} \\quad \\text{(by Equation 37)}\n",
    "\\end{align*}\n",
    "\n",
    "As if by magic, most of the terms have disappeared. It is no longer necessary to store old search vectors to ensure the $A$-orthogonality of new search vectors. This major advance is what makes CG as important an algorithm as it is, because both the space complexity and time complexity per iteration are reduced from\n",
    "$\\mathcal{O}(n^2)$ to $\\mathcal{O}(m)$, where $m$ is the number of nonzero entries of $A$. Henceforth, I shall use the abbreviation $\\beta_{[i]} = \\beta_{i,i-1}$. Simplifying further:\n",
    "\n",
    "\\begin{align*}\n",
    "\\beta_{[i]} &= \\frac{r^T_{[i]}r_{[i]}}{d^T_{[i-1]}r_{[i-1]}} & \\text{(by Equation 32)}\\\\\n",
    "&= \\frac{r^T_{[i]}r_{[i]}}{r^T_{[i-1]}r_{[i-1]}} & \\text{(by Equation 42).}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66781a42661a447fa42ceaaa48ebaa51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig30(A, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 30: The method of Conjugate Gradients.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s put it all together into one piece now. The method of Conjugate Gradients is:\n",
    "\n",
    "\\begin{align*}\n",
    "d_{[0]} &= r_{[0]} = b - Ax_{[0]} \\tag{45} \\\\\n",
    "\\alpha_{[i]} &= \\frac{r^T_{[i]}r_{[i]}}{d^T_{[i]}Ad_{[i]}} \\qquad \\text{(by Equations 32 and 42),} \\tag{46} \\\\\n",
    "x_{[i+1]} &= x_{[i]} + \\alpha_{[i]}d_{[i]}, \\\\\n",
    "r_{[i+1]} &= r_{[i]} - \\alpha_{[i]}Ad_{[i]}, \\tag{46} \\\\\n",
    "\\beta_{[i+1]} &= \\frac{r^T_{[i+1]}r_{[i+1]}}{r^T_{[i]}r_{[i]}}, \\tag{48}\\\\\n",
    "d_{[i+1]} &= r_{[i+1]} + \\beta_{[i+1]}d_{[i]}. \\tag{49}\n",
    "\\end{align*}\n",
    "\n",
    "The performance of CG on our sample problem is demonstrated in Figure 30. The name “Conjugate Gradients” is a bit of a misnomer, because the gradients are not conjugate, and the conjugate directions are not all gradients. “Conjugated Gradients” would be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<hr style=\"height:2px;border:none;color:#228;background-color:#228;\" />\n",
    "<span style=\"color:#228\">\n",
    "\n",
    "<p>Below I have added another interactive figure for you to explore the method of Conjugate Gradients. You can change the system and the starting point.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46db78a0a3f74d7fb867861192133640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "axC = fig_C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a51f3b6bbf2495ebb134954dcd9c4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Box(children=(Box(children=(FloatSlider(value=3.0, max=10.0, min=-10.0), FloatSlider(value=2.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sliders_figC(axC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<hr style=\"height:2px;border:none;color:#228;background-color:#228;\" />\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: [9. Convergence Analysis of Conjugate Gradients](CG09.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "widgets": {
   "state": {
    "01ab7d15215e44ddbdbe6bdebf436133": {
     "views": []
    },
    "0c882eeee81445f7ba5d91f4bddfb2a2": {
     "views": []
    },
    "0cb8531a2bc0420c8d3a1144f5614854": {
     "views": []
    },
    "0dfe857d1f3e493c97a92fd064aaa055": {
     "views": []
    },
    "39c8ced2980e42f495f8213a8d7d3eaf": {
     "views": []
    },
    "3cf1e163f9db408e96076753c2bd96b9": {
     "views": []
    },
    "3cfb23fd36124768af5556961340d347": {
     "views": []
    },
    "5700e342c0f04978a18f1f7f89f02075": {
     "views": []
    },
    "5bdc29d07f4d43d0bc1c1cd4fbb1e60c": {
     "views": []
    },
    "6502a895c15c454394931c48e5ac89fa": {
     "views": []
    },
    "6b8fb055d61447ae8abae4ce8735f738": {
     "views": []
    },
    "71b92e0c04d9468890dc4ae1f871397b": {
     "views": []
    },
    "8460ff9d8a5142b2b06df34358def634": {
     "views": []
    },
    "8bdbb80e475347f59d85f49bae491d9b": {
     "views": []
    },
    "8f53ce90529a48aaa24e1fd00753ec70": {
     "views": []
    },
    "95156064e7d74450824be02f0278cfcc": {
     "views": []
    },
    "9cad78f5497244c6809697edb6e9e5b2": {
     "views": []
    },
    "b173bd6686ca4975bba1e7df192ce4a9": {
     "views": []
    },
    "b324a09188384595b122cc2f46bee08c": {
     "views": []
    },
    "b7c11df031c944a5bee92852f2f6b9ec": {
     "views": []
    },
    "b9f5d58b8b5d45b1b14a39b7db8bdd09": {
     "views": []
    },
    "d050df82d65d45a7b231624207701606": {
     "views": []
    },
    "de18e0a189e64865b48a33c8aacaeb05": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "ef26ff734e194546b6b23acf6d2ac178": {
     "views": []
    },
    "f19551a022f245db9b592499ff3176c3": {
     "views": []
    },
    "f50b9ba8419d4b9895ac9cb9abca2dac": {
     "views": []
    },
    "f820df10e538494db4566dfea4402287": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
