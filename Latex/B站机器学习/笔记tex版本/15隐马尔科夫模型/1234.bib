
@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2020-02-27},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533},
	file = {Mnih 等。 - 2015 - Human-level control through deep reinforcement lea.pdf:D\:\\zotero\\storage\\W6JJC8ZP\\Mnih 等。 - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf}
}

@inproceedings{arulkumaran_alphastar_2019,
	address = {Prague, Czech Republic},
	series = {{GECCO} '19},
	title = {{AlphaStar}: an evolutionary computation perspective},
	isbn = {978-1-4503-6748-6},
	shorttitle = {{AlphaStar}},
	url = {https://doi.org/10.1145/3319619.3321894},
	doi = {10.1145/3319619.3321894},
	abstract = {In January 2019, DeepMind revealed AlphaStar to the world---the first artificial intelligence (AI) system to beat a professional player at the game of StarCraft II---representing a milestone in the progress of AI. AlphaStar draws on many areas of AI research, including deep learning, reinforcement learning, game theory, and evolutionary computation (EC). In this paper we analyze AlphaStar primarily through the lens of EC, presenting a new look at the system and relating it to many concepts in the field. We highlight some of its most interesting aspects---the use of Lamarckian evolution, competitive co-evolution, and quality diversity. In doing so, we hope to provide a bridge between the wider EC community and one of the most significant AI systems developed in recent times.},
	urldate = {2020-02-27},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {Association for Computing Machinery},
	author = {Arulkumaran, Kai and Cully, Antoine and Togelius, Julian},
	month = jul,
	year = {2019},
	keywords = {co-evolution, lamarckian evolution, quality diversity},
	pages = {314--315},
	file = {Arulkumaran et al_2019_AlphaStar.pdf:D\:\\zotero\\storage\\RSVWTMN4\\Arulkumaran et al_2019_AlphaStar.pdf:application/pdf}
}

@inproceedings{bellemare_distributional_2017,
	address = {Sydney, NSW, Australia},
	series = {{ICML}'17},
	title = {A {Distributional} {Perspective} on {Reinforcement} {Learning}},
	abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
	urldate = {2020-02-27},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning} - {Volume} 70},
	publisher = {JMLR.org},
	author = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
	month = aug,
	year = {2017},
	keywords = {DRL},
	pages = {449--458},
	file = {Bellemare et al_2017_A Distributional Perspective on Reinforcement Learning.pdf:D\:\\zotero\\storage\\RKMQZ7UE\\Bellemare et al_2017_A Distributional Perspective on Reinforcement Learning.pdf:application/pdf}
}

@inproceedings{liu_stein_2016,
	title = {Stein {Variational} {Gradient} {Descent}: {A} {General} {Purpose} {Bayesian} {Inference} {Algorithm}},
	shorttitle = {Stein {Variational} {Gradient} {Descent}},
	url = {http://papers.nips.cc/paper/6338-stein-variational-gradient-descent-a-general-purpose-bayesian-inference-algorithm.pdf},
	urldate = {2020-02-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Qiang and Wang, Dilin},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	keywords = {Stein},
	pages = {2378--2386},
	file = {Liu_Wang_2016_Stein Variational Gradient Descent.pdf:D\:\\zotero\\storage\\EHKQXK7N\\Liu_Wang_2016_Stein Variational Gradient Descent.pdf:application/pdf}
}

@article{__nodate,
	title = {基于深度强化学习的双足机器人斜坡步态控制方法},
	abstract = {为提高准被动双足机器人斜坡步行稳定性,本文提出了一种基于深度强化学习的准被动双足机器人步态控制方法.通过分析准被动双足机器人的混合动力学模型与稳定行走过程,建立了状态空间、动作空间、episode过程与奖励函数.在利用基于DDPG改进的Ape-X DPG算法持续学习后,准被动双足机器人能在较大斜坡范围内实现稳定行走.仿真实验表明,Ape-X DPG无论是学习能力还是收敛速度均优于基于PER的DDPG.同时,相较于能量成型控制,使用Ape-X DPG的准被动双足机器人步态收敛更迅速、步态收敛域更大,证明Ape-X DPG可有效提高准被动双足机器人的步行稳定性.},
	journal = {自动化学报},
	author = {吴晓光 and 刘绍维 and 杨磊 and 邓文强 and 贾哲恒},
	note = {ISBN: 0254-4156},
	keywords = {准被动双足机器人;深度强化学习;步态控制;步行稳定性},
	pages = {1--13}
}